{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070edfdc",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88336e50",
   "metadata": {},
   "source": [
    "## 1. Getting to know PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02981696",
   "metadata": {},
   "source": [
    "Answer: Yes way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771b5dc",
   "metadata": {},
   "source": [
    "Answer: Create an instance of the SparkContext class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0fcf6",
   "metadata": {},
   "source": [
    "Answer: Operations using DataFrames are automatically optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065341b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a79d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = spark.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c73bc80",
   "metadata": {},
   "source": [
    "## 2. Manipulating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights = spark.table('flights')\n",
    "\n",
    "# Show the head\n",
    "flights.show()\n",
    "\n",
    "# Add duration_hrs\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73c83e",
   "metadata": {},
   "source": [
    "Answer: SELECT dest, tail_num FROM flights WHERE air_time > 600;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fda46e",
   "metadata": {},
   "source": [
    "Answer: The average length of each airline's flights from SEA and from PDX in hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = flights.filter(\"distance > 1000\")\n",
    "\n",
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "\n",
    "# Print the data to check they're equal\n",
    "long_flights1.show()\n",
    "long_flights2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af392c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "\n",
    "# Define first filter\n",
    "filterA = flights.origin == \"SEA\"\n",
    "\n",
    "# Define second filter\n",
    "filterB = flights.dest == \"PDX\"\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ac870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == 'PDX').groupBy().min('distance').show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of air time\n",
    "flights.filter(flights.origin == 'SEA').groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd747c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average duration of Delta flights\n",
    "flights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
    "\n",
    "# Total hours in the air\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy(\"tailnum\")\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show()\n",
    "\n",
    "# Group by origin\n",
    "by_origin = flights.groupBy(\"origin\")\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import  pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy('month', 'dest')\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg('dep_delay').show()\n",
    "\n",
    "# Standard deviation of departure delay\n",
    "by_month_dest.agg(F.stddev('dep_delay')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1282f0f",
   "metadata": {},
   "source": [
    "Answer: There is only one kind of join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data\n",
    "print(airports.show())\n",
    "\n",
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on='dest', how='leftouter')\n",
    "\n",
    "# Examine the new DataFrame\n",
    "print(flights_with_airports.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf9cc5",
   "metadata": {},
   "source": [
    "## 3. Getting started with machine learning pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17449cab",
   "metadata": {},
   "source": [
    "Answer: Spark's algorithms give better results than other algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f86f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year column\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')\n",
    "\n",
    "# Join the DataFrames\n",
    "model_data = flights.join(planes, on='tailnum', how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fcaac",
   "metadata": {},
   "source": [
    "Answer: Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e4d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b288542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "\n",
    "# Convert to an integer\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n",
    "\n",
    "# Remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4ffde",
   "metadata": {},
   "source": [
    "Answer: Spark can only model numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aea67",
   "metadata": {},
   "source": [
    "Answer: By evaluating your model with a test set you can get a good idea of performance on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b312c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c19f27",
   "metadata": {},
   "source": [
    "## 4. Model tuning and selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ecd97",
   "metadata": {},
   "source": [
    "Answer: They improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae204ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceafee2",
   "metadata": {},
   "source": [
    "Answer: The model's error on held out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd27072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                         estimatorParamMaps=grid,\n",
    "                         evaluator=evaluator\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf50d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ecd4d",
   "metadata": {},
   "source": [
    "Answer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
