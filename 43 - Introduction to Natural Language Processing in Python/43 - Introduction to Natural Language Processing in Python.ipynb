{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca009ed",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab111f4",
   "metadata": {},
   "source": [
    "## 1. Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4f7fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_string = \"Let's write RegEx!\"\n",
    "PATTERN = r\"\\w+\"\n",
    "\n",
    "re.findall(PATTERN, my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb10b32",
   "metadata": {},
   "source": [
    "Answer: PATTERN = r\"\\w+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6594ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce446812",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one = \"\"\"\n",
    "SCENE 1: [wind] [clop clop clop] \n",
    "KING ARTHUR: Whoa there!  [clop clop clop] \n",
    "SOLDIER #1: Halt!  Who goes there?\n",
    "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
    "SOLDIER #1: Pull the other one!\n",
    "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
    "SOLDIER #1: What?  Ridden on a horse?\n",
    "ARTHUR: Yes!\n",
    "SOLDIER #1: You're using coconuts!\n",
    "ARTHUR: What?\n",
    "SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n",
    "ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\n",
    "SOLDIER #1: Where'd you get the coconuts?\n",
    "ARTHUR: We found them.\n",
    "SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\n",
    "ARTHUR: What do you mean?\n",
    "SOLDIER #1: Well, this is a temperate zone.\n",
    "ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\n",
    "SOLDIER #1: Are you suggesting coconuts migrate?\n",
    "ARTHUR: Not at all.  They could be carried.\n",
    "SOLDIER #1: What?  A swallow carrying a coconut?\n",
    "ARTHUR: It could grip it by the husk!\n",
    "SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\n",
    "ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\n",
    "SOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\n",
    "ARTHUR: Please!\n",
    "SOLDIER #1: Am I right?\n",
    "ARTHUR: I'm not interested!\n",
    "SOLDIER #2: It could be carried by an African swallow!\n",
    "SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\n",
    "SOLDIER #2: Oh, yeah, I agree with that.\n",
    "ARTHUR: Will you ask your master if he wants to join my court at Camelot?!\n",
    "SOLDIER #1: But then of course a-- African swallows are non-migratory.\n",
    "SOLDIER #2: Oh, yeah...\n",
    "SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \n",
    "SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\n",
    "SOLDIER #1: No, they'd have to have it on a line.\n",
    "SOLDIER #2: Well, simple!  They'd just use a strand of creeper!\n",
    "SOLDIER #1: What, held under the dorsal guiding feathers?\n",
    "SOLDIER #2: Well, why not?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa351f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length', 'Please', 'maintain', 'Pendragon', 'house', 'it', 'order', 'Listen', 'Camelot', 'they', 'the', '2', \"'s\", 'wants', \"'m\", 'ridden', 'SOLDIER', 'kingdom', 'swallow', 'if', 'join', 'wind', 'Are', 'speak', 'of', 'one', 'Saxons', 'carry', 'agree', 'bangin', 'may', 'horse', 'land', 'No', 'not', 'he', 'strand', 'these', 'beat', 'winter', 'found', 'wings', 'grip', 'its', 'just', '--', 'lord', 'yet', 'why', 'anyway', 'carried', 'Wait', 'do', \"n't\", 'ratios', 'together', 'Mercea', 'Court', 'use', 'plover', 'a', 'What', 'but', 'are', 'using', 'strangers', 'knights', 'Am', 'Arthur', 'all', \"'\", 'covered', 'who', 'coconuts', \"'em\", 'search', 'matter', 'held', 'ask', ',', 'with', 'warmer', 'carrying', 'that', 'times', 'at', 'Britons', 'The', '?', '.', 'my', 'to', 'from', 'breadth', 'zone', 'grips', 'you', 'yeah', 'maybe', 'me', 'pound', 'Will', 'ARTHUR', 'got', 'and', '#', 'them', 'velocity', 'forty-three', 'halves', 'feathers', '...', 'master', 'fly', 'African', 'right', 'A', 'be', 'Ridden', 'an', 'your', 'coconut', 'European', 'KING', 'King', 'Where', 'clop', 'seek', 'or', 'ounce', 'does', ']', 'point', 'question', 'under', 'Patsy', 'Well', 'swallows', 'tropical', 'mean', 'tell', 'temperate', 'Who', 'then', 'I', 'snows', 'through', 'where', '!', 'trusty', 'is', 'empty', 'weight', 'five', 'bring', 'climes', 'sun', 'every', 'Supposing', 'son', 'martin', 'second', 'servant', 'our', 'migrate', 'back', 'Not', 'other', 'simple', 'will', 'Whoa', 'could', 'two', 'by', 'creeper', 'Pull', 'So', 'interested', 'husk', 'In', 'there', 'sovereign', ':', 'suggesting', 'course', 'since', \"'d\", 'They', 'It', '1', 'on', 'Found', 'But', 'SCENE', 'defeator', 'dorsal', \"'ve\", 'get', 'bird', 'non-migratory', 'am', 'castle', 'needs', 'minute', 'We', 'That', \"'re\", '[', 'go', 'air-speed', 'Uther', 'court', 'south', 'Halt', 'You', 'guiding', 'England', 'here', 'must', 'line', 'in', 'this', 'have', 'Yes', 'goes', 'Oh'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60449ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581 589\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c461df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(10, 33), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[(.*)\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334278e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"^.*?:\"\n",
    "# pattern2 = r\"[^.]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47273ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "pattern1 = r'(\\w+|\\?|!)'\n",
    "pattern2 = r'(\\w+|#\\d|\\?|!)'\n",
    "pattern3 = r'(#\\d\\w+\\?!)'\n",
    "pattern4 = r'\\s+'\n",
    "\n",
    "regexp_tokenize(my_string, pattern2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca1b32",
   "metadata": {},
   "source": [
    "Answer: r'(\\w+|#\\d|\\?|!)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f49fcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python', \n",
    "          '#NLP is super fun! <3 #learning', \n",
    "          'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5022822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([\\@|\\#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597c49eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04267f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbe0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Monty Python and the Holy Grail.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    holy_grail = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "150008e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, r'\\w+') for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a5172",
   "metadata": {},
   "source": [
    "## 2. Simple topic identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f954b3",
   "metadata": {},
   "source": [
    "Answer: ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13131348",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Wikipedia articles/wiki_text_debugging.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    article = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed7d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 69), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90f737",
   "metadata": {},
   "source": [
    "Answer: Lemmatization, lowercasing, removing unwanted tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12eb5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'english_stopwords.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    english_stopwords = file.read()\n",
    "english_stops = english_stopwords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0d49ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 39), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aec469",
   "metadata": {},
   "source": [
    "Answer: Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80202b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf8e7e",
   "metadata": {},
   "source": [
    "Answer: (5 / 100) * log(200 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ecfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15601735",
   "metadata": {},
   "source": [
    "## 3. Named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f64266b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'News articles/uber_apple.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    article = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85a25371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Erturk Memmedli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Erturk\n",
      "[nltk_data]     Memmedli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Erturk\n",
      "[nltk_data]     Memmedli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1d2f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef34bc",
   "metadata": {},
   "source": [
    "Answer: NLTK, the Stanford Java Libraries and some environment variables to help with integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6273b",
   "metadata": {},
   "source": [
    "Answer: NORP, CARDINAL, MONEY, WORK OF ART, LANGUAGE, EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "file_path = 'french.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    article = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d53112",
   "metadata": {},
   "source": [
    "## 4. Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00de1c",
   "metadata": {},
   "source": [
    "Answer: All of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad2202",
   "metadata": {},
   "source": [
    "Answer: Both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24483de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b33583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "#print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3912edee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c70f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(tfidf_df.columns) - set(count_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd368751",
   "metadata": {},
   "source": [
    "Answer: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71554546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2146be80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93a011",
   "metadata": {},
   "source": [
    "Answer: All of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7df3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcfe07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.280753302177917, '00000031'), (-11.280753302177917, '00006'), (-11.280753302177917, '000ft'), (-11.280753302177917, '001'), (-11.280753302177917, '002'), (-11.280753302177917, '003'), (-11.280753302177917, '006'), (-11.280753302177917, '008'), (-11.280753302177917, '010'), (-11.280753302177917, '013'), (-11.280753302177917, '025'), (-11.280753302177917, '027'), (-11.280753302177917, '035'), (-11.280753302177917, '037'), (-11.280753302177917, '040'), (-11.280753302177917, '044'), (-11.280753302177917, '048'), (-11.280753302177917, '066'), (-11.280753302177917, '068'), (-11.280753302177917, '075')]\n",
      "REAL [(-8.036772745824807, 'president'), (-8.022187159522364, 'american'), (-8.013319806154513, 'media'), (-8.007761560290644, 'donald'), (-8.006632122322646, 'october'), (-7.989623223030759, 'government'), (-7.929695447721539, 'like'), (-7.922750601304927, 'war'), (-7.915731838943572, 'new'), (-7.908889774759155, 'world'), (-7.885018054191407, 'just'), (-7.758145325115569, 'said'), (-7.7498037548099585, 'russia'), (-7.697669509488481, 'fbi'), (-7.604825769578616, '2016'), (-7.554879292243166, 'election'), (-7.541640806988918, 'people'), (-7.235945549755579, 'hillary'), (-6.923220068888362, 'clinton'), (-6.867377223688766, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.feature_log_prob_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
